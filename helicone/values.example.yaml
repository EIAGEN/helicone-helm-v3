################################################################################
#
#                                 HELICONE
#
################################################################################
global:
  # Postgresql helm chart expects this format
  postgresql:
    auth:
      database: helicone_test
      username: postgres
      postgresPassword: "your-super-secret-and-long-postgres-password"

primary:
  persistence:
    size: 5Gi
    storageClass: "gp2-immediate"

helicone:
  web:
    enabled: true
    image:
      repository: 849596434884.dkr.ecr.us-east-2.amazonaws.com/helicone/web
      pullPolicy: IfNotPresent
      tag: "v2025.05.23"
    replicaCount: 1
    service:
      annotations: {}
      type: ClusterIP
      port: 3000
    extraEnvVars: []
    ingress:
      enabled: true
      className: "nginx"
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
      tls:
        - secretName: helicone-tls
          hosts:
            - heliconetest.com
      hosts:
        - host: heliconetest.com
          paths:
            - path: /
              pathType: Prefix
              backend:
                service:
                  name: helicone-web
                  port:
                    number: 3000
    resources:
      requests:
        cpu: 100m
        memory: 256Mi
      limits:
        cpu: 500m
        memory: 1Gi
    verticalPodAutoscaler:
      enabled: false
      updateMode: "Off"
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
      maxUnavailable: null
    autoscaling:
      enabled: true
      minReplicas: 2
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          percentPolicy: 50
          periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          percentPolicy: 100
          periodSeconds: 15
    postgresMigrationRunner:
      image:
        repository: 849596434884.dkr.ecr.us-east-2.amazonaws.com/helicone/migrations
        pullPolicy: IfNotPresent
        tag: "v2025.05.23-3"
      resources: {}

  clickhouse:
    enabled: true
    image:
      repository: clickhouse/clickhouse-server
      pullPolicy: IfNotPresent
      tag: "23.4.2.11"
    replicaCount: 1
    service:
      annotations: {}
      type: ClusterIP
      port: 8123
    resources: {}
    persistence:
      storageClass: "gp2-immediate"
      size: 10Gi

  minio:
    enabled: true
    image:
      repository: minio/minio
      pullPolicy: IfNotPresent
      tag: "RELEASE.2023-05-18T00-05-36Z"
    replicaCount: 1
    service:
      annotations: {}
      type: ClusterIP
      port: 9000
      consolePort: 9001
    resources: {}
    persistence:
      storageClass: "gp2-immediate"
      size: 20Gi
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
    setup:
      image:
        repository: minio/mc
        tag: "latest"
        pullPolicy: IfNotPresent
      buckets:
        - "request-response-storage"

  jawn:
    enabled: true
    image:
      repository: 849596434884.dkr.ecr.us-east-2.amazonaws.com/helicone/jawn
      pullPolicy: IfNotPresent
      tag: "v2025.05.19"
    replicaCount: 1
    service:
      annotations: {}
      type: ClusterIP
      port: 8585
    extraEnvVars: []
    publicUrl: "https://heliconetest.com/jawn"
    ingress:
      enabled: false
      ingressClassName: ""
      annotations: {}
    resources:
      requests:
        cpu: 50m
        memory: 128Mi
      limits:
        cpu: 250m
        memory: 512Mi
    autoscaling:
      enabled: true
      minReplicas: 1
      maxReplicas: 10
      targetCPUUtilizationPercentage: 80
      targetMemoryUtilizationPercentage: 80
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          percentPolicy: 50
          periodSeconds: 60
        scaleUp:
          stabilizationWindowSeconds: 60
          percentPolicy: 100
          periodSeconds: 15

  config:
    vercelEnv: "development"
    clickhouseHost: "helicone-clickhouse"
    clickhousePort: "8123"
    clickhouseUser: "default"
    s3BucketName: "helm-request-response-storage"
    s3Endpoint: "https://s3.us-west-2.amazonaws.com"
    openaiProxyPort: "8787"
    heliconeApiPort: "8788"
    anthropicProxyPort: "8789"
    gatewayApiPort: "8790"
    jawnPort: "8585"
    jawnPublicUrl: "http://localhost:8585"
    kongHttpPort: "8000"
    kongHttpsPort: "8443"
    pgrstDbSchemas: "public,storage,graphql_public"
    siteUrl: "http://localhost:3000"
    additionalRedirectUrls: ""
    jwtExpiry: "3600"
    disableSignup: "false"
    apiExternalUrl: "http://localhost:8000"
    mailerUrlpathsConfirmation: "/auth/v1/verify"
    mailerUrlpathsInvite: "/auth/v1/verify"
    mailerUrlpathsRecovery: "/auth/v1/verify"
    mailerUrlpathsEmailChange: "/auth/v1/verify"
    enableEmailSignup: "true"
    enableEmailAutoconfirm: "false"
    enablePhoneSignup: "true"
    enablePhoneAutoconfirm: "true"
    studioDefaultOrganization: "Default Organization"
    studioDefaultProject: "Default Project"
    studioPort: "3000"
    imgproxyEnableWebpDetection: "true"
    functionsVerifyJwt: "false"
    dockerSocketLocation: "/var/run/docker.sock"
    googleProjectId: "GOOGLE_PROJECT_ID"
    googleProjectNumber: "GOOGLE_PROJECT_NUMBER"
    nodeEnv: "development"

  secrets:
    betterAuthSecret: "your-better-auth-secret-key-change-this-in-production"
    stripeSecretKey: "your-stripe-secret-key-change-this-in-production"
    minioRootUser: "minioadmin"
    minioRootPassword: "your-super-secret-minio-password-change-this-in-production"
    user: "default"
    s3AccessKey: "your-s3-access-key-change-this-in-production"
    s3SecretKey: "your-s3-secret-key-change-this-in-production"

mailhog:
  enabled: true
  image:
    repository: mailhog/mailhog
    tag: latest
    pullPolicy: IfNotPresent
  resources:
    requests:
      memory: "128Mi"
      cpu: "100m"
    limits:
      memory: "256Mi"
      cpu: "200m"

extraObjects:
  - apiVersion: networking.k8s.io/v1
    kind: Ingress
    metadata:
      name: helicone-services-ingress
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        nginx.ingress.kubernetes.io/rewrite-target: /$2
        nginx.ingress.kubernetes.io/proxy-body-size: "100m"
    spec:
      ingressClassName: nginx
      rules:
        - host: heliconetest.com
          http:
            paths:
              - backend:
                  service:
                    name: helicone-jawn
                    port:
                      number: 8585
                path: /jawn(/|$)(.*)
                pathType: ImplementationSpecific
      tls:
        - hosts:
            - heliconetest.com
          secretName: helicone-tls
clusterAutoscaler:
  enabled: true
  image:
    tag: "v1.29.2"
  clusterName: "helicone"
  extraArgs: []
  serviceAccount:
    roleArn: "arn:aws:iam::849596434884:role/cluster-autoscaler-role"

beyla:
  enabled: true
  image:
    repository: grafana/beyla
    tag: "latest"
    pullPolicy: IfNotPresent
  securityContext:
    privileged: true
    capabilities:
      sysAdmin: false
  otel:
    endpoint: ""
    headers: ""
    protocol: "http/protobuf"
  config:
    logLevel: "info"
    services:
      openPorts: "3000,8585"
    routes:
      unmatched: "heuristic"
      patterns:
        - "/healthcheck"
        - "/api/v1/{id}"
        - "/auth/{action}"
    tracing:
      enabled: true
      sampler:
        name: "always_on"
        arg: "1.0"
    metrics:
      interval: "15s"
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      cpu: 200m
      memory: 256Mi
  extraEnvVars:
    BEYLA_PRINT_TRACES: "true"
    BEYLA_PROMETHEUS_PORT: "9090"

# Kube-Prometheus-Stack Configuration
monitoring:
  # Deploy monitoring components to a separate namespace
  namespaceOverride: "monitoring"

  # Prometheus Operator
  prometheusOperator:
    enabled: true
    # Deploy operator to monitoring namespace
    namespaces:
      releaseNamespace: true
      additional:
        - monitoring
    # Resource limits for the operator
    resources:
      limits:
        cpu: 200m
        memory: 200Mi
      requests:
        cpu: 100m
        memory: 100Mi

  # Prometheus Configuration
  prometheus:
    enabled: true
    prometheusSpec:
      # Retention period for metrics
      retention: 30d
      retentionSize: "10GB"

      # Resource allocation for Prometheus
      resources:
        requests:
          cpu: 500m
          memory: 2Gi
        limits:
          cpu: 2000m
          memory: 8Gi

      # Storage configuration
      storageSpec:
        volumeClaimTemplate:
          spec:
            storageClassName: "gp2-immediate"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 50Gi

      # Service monitors to discover across all namespaces
      serviceMonitorSelector: {}
      serviceMonitorNamespaceSelector: {}
      podMonitorSelector: {}
      podMonitorNamespaceSelector: {}

      # Enable cross-namespace monitoring
      ruleSelector: {}
      ruleNamespaceSelector: {}

      # Enable admin API (useful for metric deletion)
      enableAdminAPI: true

  # Grafana Configuration
  grafana:
    enabled: true

    # Deploy to monitoring namespace
    namespaceOverride: "monitoring"

    # Admin credentials
    adminPassword: "admin" # Change this in production!

    # Persistence for Grafana
    persistence:
      enabled: true
      storageClassName: "gp2-immediate"
      size: 10Gi
      accessModes:
        - ReadWriteOnce

    # Resource allocation
    resources:
      limits:
        cpu: 500m
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi

    # Grafana configuration
    grafana.ini:
      server:
        root_url: "%(protocol)s://%(domain)s:%(http_port)s/grafana"
        serve_from_sub_path: true
      auth:
        disable_login_form: false
      users:
        auto_assign_org_role: Editor
      dashboards:
        default_home_dashboard_path: /var/lib/grafana/dashboards/default/kubernetes-cluster-monitoring.json

    # Sidecar for dashboard provisioning
    sidecar:
      dashboards:
        enabled: true
        label: grafana_dashboard
        # Search for dashboards across all namespaces
        searchNamespace: ALL
        provider:
          allowUiUpdates: true
      datasources:
        enabled: true
        defaultDatasourceEnabled: true
        # Search for datasources across all namespaces
        searchNamespace: ALL

    # Service configuration
    service:
      type: ClusterIP # Using ClusterIP since you have ingress
      port: 80
      targetPort: 3000

    # Ingress configuration for Grafana - deploy in monitoring namespace
    ingress:
      enabled: true
      ingressClassName: "nginx"
      annotations:
        cert-manager.io/cluster-issuer: letsencrypt-prod
        nginx.ingress.kubernetes.io/rewrite-target: /$2
      hosts:
        - heliconetest.com
      path: /grafana(/|$)(.*)
      tls:
        - secretName: helicone-grafana-tls
          hosts:
            - heliconetest.com

  # AlertManager Configuration
  alertmanager:
    enabled: true

    # Deploy to monitoring namespace
    namespaceOverride: "monitoring"

    alertmanagerSpec:
      storage:
        volumeClaimTemplate:
          spec:
            storageClassName: "gp2-immediate"
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 10Gi
      resources:
        requests:
          cpu: 100m
          memory: 128Mi
        limits:
          cpu: 200m
          memory: 256Mi

    # Basic alert routing configuration
    config:
      global:
        resolve_timeout: 5m
      route:
        group_by: ["alertname", "cluster", "service"]
        group_wait: 10s
        group_interval: 10s
        repeat_interval: 12h
        receiver: "null"
        routes:
          - match:
              alertname: Watchdog
            receiver: "null"
      receivers:
        - name: "null"

  # Node Exporter for node metrics
  nodeExporter:
    enabled: true

  # Kube State Metrics for Kubernetes object metrics
  kubeStateMetrics:
    enabled: true

  # Enable pre-configured ServiceMonitors
  kubeApiServer:
    enabled: true
  kubeControllerManager:
    enabled: true
  kubeScheduler:
    enabled: true
  kubeEtcd:
    enabled: true
  kubelet:
    enabled: true
  kubeProxy:
    enabled: true

  # Additional Prometheus rules
  additionalPrometheusRulesMap:
    custom-rules:
      groups:
        - name: kubernetes-apps
          interval: 15s
          rules:
            - alert: PodCrashLooping
              expr: |
                rate(kube_pod_container_status_restarts_total[5m]) > 0
              for: 5m
              labels:
                severity: critical
              annotations:
                summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
                description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} has been restarting frequently"

            - alert: HighMemoryUsage
              expr: |
                (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.9
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High memory usage on {{ $labels.instance }}"
                description: "Memory usage is above 90% on {{ $labels.instance }}"

            - alert: HighCPUUsage
              expr: |
                100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
              for: 5m
              labels:
                severity: warning
              annotations:
                summary: "High CPU usage on {{ $labels.instance }}"
                description: "CPU usage is above 80% on {{ $labels.instance }}"
